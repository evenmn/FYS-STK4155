\section{Code} \label{sec:code}
The code is mainly implemented in Python, due to its neatness and flexibility. The numpy package has a lot of functions which are both fast and convenient for machine learning purposes, and there exist some packages that provide easy and fast machine learning tools such as Scikit-Learn, Keras and Tensorflow. However, the neural networks that we implement from scratch will not be as fast in Python as in a low-level language, and they where therefore implemented in C++ as well. 

\subsection{Code structure} \label{sec:structure}
In order to reuse the code where possible, we ended up with several functions that communicate in criss-cross. The main functions are called \textit{find\_energy.py} and \textit{classifier.py}, where the former estimates the energy of a Ising lattice using linear regression and neural network, getting the data from \textit{Ising\_1D.py}. The latter estimates the phase of the Ising model using logistic regression and neural networks, getting the data from \textit{Ising\_2D.py} (taken from [3]). \textit{error\_tools.py} consists of functions that returns the error, including MSE, R2 and Accuracy, which communicates with \textit{resampling.py} as well. Both main functions are largely dependent on \textit{neural\_network.py}, which gets the optimization functions from \textit{optimization.py} and the activation functions from \textit{activation.py}.
\begin{figure} [H]
\centering
\begin{tikzpicture}[auto,node distance=0.9cm]
  \begin{scope}[scale=0.7, transform shape]
	  % Create main nodes
	  \node[entity] (node1) [top color= myblue, bottom color=myblue] {find energy};
	  \node[entity] (rel1) [below right = of node1, top color= myblue, bottom color=myblue] {linear regression};
	  \node[entity] (node2) [above right = of rel1, top color= myblue, bottom color=myblue] {classifier};
	  \node[entity] (rel2) [below = of rel1, level distance=0.1cm, top color= myblue, bottom color=myblue] {Error tools};
	  \node[attribute] (att1) [below left = of node1, top color= myblue, bottom color=myblue] {Ising 1D};
	  \node[attribute] (att2) [below right = of node2, top color= myblue, bottom color=myblue] {Ising 2D};
	  \node[entity] (rel3) [below = of rel2, top color= myblue, bottom color=myblue] {Resampling};
	  \node[relationship] (rel4) [below = of rel3, top color= myblue, bottom color=myblue] {Neural Network};
	  \node[attribute] (att3) [right = of rel4, top color= myblue, bottom color=myblue] {Optimization};
	  \node[attribute] (att4) [left = of rel4, top color= myblue, bottom color=myblue] {Activation};
	  
	  
	  % Draw paths
	  \path (node1) edge node {} (att1);
	  \path (node2) edge node {} (att2);
	  \path (node1) edge node {} (rel1);
	  \path (node1) edge node {} (rel2);
	  \path (node1) edge node {} (rel3);
	  \path (node1) edge node {} (rel4);
	  \path (node2) edge node {} (rel2);
	  \path (node2) edge node {} (rel4);
	  \path (rel4) edge node {} (att3);
	  \path (rel4) edge node {} (att4);
	  \path (rel4) edge node {} (rel3);
	  \path (rel3) edge node {} (rel2);
  \end{scope}

\end{tikzpicture}
\caption{Code structure}
\label{fig:codestructure}
\end{figure}

\subsection{Run the code}
The neural network was implemented generally, and should be able to handle an arbitrary number of hidden layers and optional hyper parameters and activation functions. However, a few assumptions were made to maintain some neatness, for instance we assume that all hidden layers are affected by the same activation function, and the cost function is fixed apart from the regularization parameter. By default, the neural network class takes a set of input arrays $X$, corresponding targets $t$, number of iterations $T$, number of hidden nodes $h$, which is either an integer or a list of integer if one want multiple hidden layers. No hidden layer by default. $eta$ is the learning rate and $lamb$ is the regularization parameter, both set to 1e-4, and $f1$ and $f2$ are the output activation function and the activation function on hidden layers respectively. They are optional functions, and in \textit{activation.py} the \textit{pure linear}, \textit{logistic}, \textit{tanh}, \textit{ReLU}, \textit{Leaky ReLU} and \textit{ELU} functions are implemented. Finally, $opt$ is the optimizer, which is GD by default, but also SGD is implemented in \textit{optimization.py}.

\lstset{basicstyle=\scriptsize}
\begin{lstlisting}
NeuralNetwork(X, t, T, h=0, eta=1e-4, lamb=1e-4, f1=none, f2=none, opt=GD)
\end{lstlisting}
By calling the solver, the network will minimize the weights with respect to the cost function and return the weights. The weight matrix contains the bias weights as well, which saves us from a few lines of code. To recall outputs, the recall function is called given an arbitrary input with the same shape as the input which was used in training. A general call could look like

\lstset{basicstyle=\scriptsize}
\begin{lstlisting}
import numpy as np
import neural_network as nn
from activation import *
from optimization import *

# Data set
X_train = np.array([...])         # Inputs
t_train = np.array([...])         # Targets
X_test = np.array([...])          # Inputs
t_test = np.array([...])          # Targets

# Parameters
T = 10          # Number of iterations
h = [10,10]     # Number of hidden nodes
eta = 1e-4      # Learning rate
lamb = 1e-4     # Regularization parameter

# Activation functions
f1 = logistic       # Activation on output layer
f2 = Leaky_ReLU	    # Activation on hidden layer(s)
opt = SGD           # Minimization function

obj = nn.NeuralNetwork(X_train, t_train, T, h, eta, lamb, f1, f2, opt)
obj.solver()                                      # Obtain optimal weights
y_train = obj.recall(X_train)                     # Recall training phase
y_test = obj.recall(X_test)                       # Recall test phase
Error = error_estimator(t_test, y_test)           # Error estimation
\end{lstlisting}

\subsection{Implementation}
The algorithm used is based on the theory in section \ref{sec:neural_network}, where we vectorize to maximize the computational speed. For the forward phase, the implementation looks like
\lstset{basicstyle=\scriptsize}
\begin{lstlisting}
'''Feed forward'''
out = [np.insert(X, 0, 1)]  	              # Add bias
for i in range(H):     		                  # Loop over hidden layers
	net = np.dot(out[-1], W[i])     		  # Net output to layer i
	Out = f2(net)                        	  # Output to layer i
	out.append(np.insert(Out, 0, 1))     	  # Add bias
net = np.dot(out[-1], W[-1])        		  # Final output
out.append(f1(net))                 		  # Final output with f1
\end{lstlisting}
where all the "self"'s from the object orientation are removed due to legibility. Similarly, the backward propagation can be implemented like this
\begin{lstlisting}
'''Backward propagation'''
deltah = [(out[-1] - t) * df1(out[-1])]
for j in range(H):
	delta = W[H-j].dot((deltah[-1]).T) * df2(self.out[H-j])
	deltah.append(delta[:-1])
deltah=deltah[::-1]
\end{lstlisting}