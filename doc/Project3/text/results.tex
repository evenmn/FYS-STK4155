\section{Results} \label{sec:results}
Finally we are ready to present the results, where the accuracy score is in our focus. We will evaluate each method separately, starting with the logistic regression and moving on to the neural network based methods. What is common for all methods, is that the softmax function was used as output activation function and ADAM was used as minimization tool with a batch size of 32 and a various number of epochs.

\subsection{Logistic regression}
For logistic regression, we examined how the accuracy was dependent on the regularization parameter with the values $10^{-1}$, $10^{-3}$, $10^{-5}$ and 0. In addition, the number of epochs was varied in the range $[10,50]$. The results can be found below in table \eqref{tab:logistic}.
\begin{table} [H]
	\caption{The accuracy-score for the training set (Train) and test set (Test) with a changing regularization parameter. The number of epochs was set to 10, 20, 30, 40 and 50. As optimization tool, ADAM was used, and we used the softmax activation function.}
	\begin{tabularx}{\textwidth}{c|XX:XX:XX:XX} \hline\hline
		\label{tab:logistic}
		\textbf{Epochs}& \multicolumn{8}{c}{\textbf{Regularization}}\\ \hline
		&\multicolumn{2}{c}{0.1}&\multicolumn{2}{c}{0.001}&\multicolumn{2}{c}{0.00001}&\multicolumn{2}{c}{0}\\ \hline
		& Train & Test & Train & Test & Train & Test & Train & Test\\ \hline
		10 & 0.3925 & 0.4112 & 0.2000 & 0.2042 & 0.2543 & 0.2447 & 0.2308 & 0.2429\\
		20 & 0.4572 & 0.4802 & 0.2927 & 0.2843 & 0.3320 & 0.3238 & 0.4135 & 0.3919\\
		30 & 0.5741 & 0.5842 & 0.2948 & 0.2815 & 0.3387 & 0.3395 & 0.4393 & 0.4186\\
		40 & 0.5780 & 0.5603 & 0.2948 & 0.2861 & 0.3433 & 0.3385 & 0.4402 & 0.4103\\
		50 & \textbf{0.5916} & \textbf{0.5998} & 0.3007 & 0.2833 & 0.3389 & 0.3395 & 0.4590 & 0.4453\\ \hline\hline
	\end{tabularx}
\end{table}
As we can see, the more epochs the higher accuracy, but when increasing the number of epochs further, the score does not improve much. Apparently, there is no such linear dependency when it comes to the regularization, but we observe that 0.1 works best. 

\subsection{Feed-forward Neural Networks}
We found the FNNs to work best without regularization, and will not present results with various regularization parameters. Instead, we vary the number of hidden layers, where each has 1024 nodes. Also here we present the accuracy-score for different number of epochs, going from 20 to 100. See table \eqref{tab:fnn} for the actual accuracy.
\begin{table} [H]
	\caption{The accuracy-score for the training set (Train) and test set (Test) with 1-4 hidden layers with 1024 nodes each. The number of epochs was set to 20, 40, 60, 80 and 100. As optimization tool, ADAM was used, with a batch size of 32. On the output layer we used softmax activation, and on the hidden layers the logistic function was used. A dropout of 50\% was used in all layers.}
	\begin{tabularx}{\textwidth}{c|XX:XX:XX:XX} \hline\hline
		\label{tab:fnn}
		\textbf{Epochs}& \multicolumn{8}{c}{\textbf{Hidden nodes}}\\ \hline
		&\multicolumn{2}{c}{1024}&\multicolumn{2}{c}{2x1024}&\multicolumn{2}{c}{3x1024}&\multicolumn{2}{c}{4x1024}\\ \hline
		&Train&Test&Train&Test&Train&Test&Train&Test\\ \hline
		20& 0.8684 & 0.8464 & 0.8603 & 0.8602 & 0.8541 & 0.8602 & 0.8341 & 0.8298\\
		40& 0.9245 & 0.8924 & 0.9204 & 0.8942 & 0.9195 & 0.8868 & 0.8990 & 0.8924\\
		60& 0.9416 & 0.9062 & 0.9508 & 0.9154 & 0.9317 & 0.9016 & 0.9254 & 0.8960\\
		80& 0.9572 & 0.8942 & 0.9556 & 0.9163 & 0.9508 & 0.9172 & 0.9418 & 0.9016\\
		100& \textbf{0.9607} & 0.9016 & 0.9579 & 0.9126 & 0.9627 & 0.9181 & 0.9510 & \textbf{0.9200}\\ \hline\hline
	\end{tabularx}
\end{table}

Again we see that the more epochs the higher accuracy. We went further increasing the number of epochs to 1000, and got a training accuracy of 0.99 and a test accuracy of 0.94 for a network with three hidden layers. Apparently, the test accuracy is more sensitive to the number of hidden layers then the training accuracy. 

\subsection{Convolutional Neural Networks}
\begin{table} [H]
	\caption{}
	\begin{tabularx}{\textwidth}{c|XXXX} \hline\hline
		\label{tab:cnn}
		\textbf{Epochs}& \multicolumn{4}{c}{\textbf{Hidden nodes}}\\ \hline
		&1024&2x1024&3x1024&4x1024\\ \hline
		
		100& 0.4371 & 0.4381 & 0.3333 & \\
		200& 0.4379 & 0.4367 & 0.3333 & \\
		300& 0.4371 & 0.4382 & 0.3333 & \\
		400& 0.9930 & 0.9929 & 0.9655 & \\
		500& 0.9935 & 0.9934 & 0.9679 & \\ \hline\hline
	\end{tabularx}
\end{table}

\subsection{Recurrent Neural Networks}
\begin{table} [H]
	\caption{}
	\begin{tabularx}{\textwidth}{c|XXXX} \hline\hline
		\label{tab:rnn}
		\textbf{Epochs}& \multicolumn{4}{c}{\textbf{Hidden nodes}}\\ \hline
		&1024&2x1024&3x1024&4x1024\\ \hline
		
		100& 0.4371 & 0.4381 & 0.3333 & \\
		200& 0.4379 & 0.4367 & 0.3333 & \\
		300& 0.4371 & 0.4382 & 0.3333 & \\
		400& 0.9930 & 0.9929 & 0.9655 & \\
		500& 0.9935 & 0.9934 & 0.9679 & \\ \hline\hline
	\end{tabularx}
\end{table}
