\section{Classification methods} \label{sec:methods}
Classification is very important in everyday life, and we often use classification even without thing about it. A simple example is when we distinguish between people, which is usually an easy task for a human. For a computer, on the other hand, classification is difficult, but fortunately some great methods are developed mainly based on neural networks. In this contest we will investigate several methods, spanning from logistic regression to various neural networks like \textbf{Feed-forward Neural Networks}, \textbf{Convolutional Neural Networks} and \textbf{Recurrent Neural Networks}. 

\subsection{Logistic regression}
Logistic regression is a linear model, where...

\begin{figure} [H]
	\centering
	\begin{tikzpicture}
	\node[functions] (center) {};
	\node[below of=center,font=\scriptsize,text width=4em] {Activation function};
	\draw[thick] (0.5em,0.5em) -- (0,0.5em) -- (0,-0.5em) -- (-0.5em,-0.5em);
	\draw (0em,0.75em) -- (0em,-0.75em);
	\draw (0.75em,0em) -- (-0.75em,0em);
	\node[right of=center] (right) {};
	\path[draw,->] (center) -- (right);
	\node[functions,left=3em of center] (left) {$\sum$};
	\path[draw,->] (left) -- (center);
	\node[weights,left=3em of left] (2) {$w_2$} -- (2) node[input,left of=2] (l2) {$x_2$};
	\path[draw,->] (l2) -- (2);
	\path[draw,->] (2) -- (left);
	\node[below of=2] (dots) {$\vdots$} -- (dots) node[left of=dots] (ldots) {$\vdots$};
	\node[weights,below of=dots] (n) {$w_n$} -- (n) node[input,left of=n] (ln) {$x_n$};
	\path[draw,->] (ln) -- (n);
	\path[draw,->] (n) -- (left);
	\node[weights,above of=2] (1) {$w_1$} -- (1) node[input,left of=1] (l1) {$x_1$};
	\path[draw,->] (l1) -- (1);
	\path[draw,->] (1) -- (left);
	\node[weights,above of=1] (0) {$b$} -- (0) node[input,left of=0] (l0) {$B$};
	\node[right of=0,font=\scriptsize] {BIAS};
	\path[draw,->] (l0) -- (0);
	\path[draw,->] (0) -- (left);
	\node[below of=ln,font=\scriptsize] {inputs};
	\node[below of=n,font=\scriptsize] {weights};
	\end{tikzpicture}
	\caption{Logistic regression model with $n$ inputs.}
	\label{fig:single_perceptron}
\end{figure}
In logistic regression, we usually have one binary output node for each class, but for two categories one output node is sufficient, which can be fired or not fired. 

Initially, one needs to train the perceptron such that it knows which outputs are correct, and for that one needs to know the outputs that correspond to the inputs. Every time the network is trained, the weights are adjusted such that the error is minimized.

The very first step is to calculate the initial outputs (forward phase), where the weights usually are set to small random numbers. Then the error is calculated, and the weights are updated to minimize the error (backward phase). So far so good.

\subsubsection*{Forward phase}\label{sec:forward}
Let us look at it from a mathematical perspective, and calculate the net output. The net output seen from an output node is simply the sum of all the "arrows" that point towards the node, see figure (\ref{fig:single_perceptron}), where each "arrow" is given by the left-hand node multiplied with its respective weight. For example, the contribution from input node 2 to the output node follows from $X_2\cdot w_{2}$, and the total net output to the output $O$ is therefore
\begin{empheq}[box={\mybluebox[5pt]}]{equation}
net = \sum_{i=1}^{I} x_i\cdot w_i + b\cdot 1.
\label{eq:forward}
\end{empheq}
Just some notation remarks: $x_i$ is the value of input node $i$ and $w_{i}$ is the weight which connects input $i$ to the output. $b$ is the bias weight, which we will discuss later.

You might wonder why we talk about the net output all the time, do we have other outputs? If we look at the network mathematically, what we talk about as the net output should be our only output. Anyway, it turns out to be convenient mapping the net output to a final output using an activation function, which is explained further in section \ref{sec:sigmoid1}. The activation function, $f$, takes in the net output and gives the output, 
\begin{equation}
out = f(net).
\end{equation}
If not everything is clear right now, it is fine. We will discuss the most important concepts before we dive into the maths.

\subsubsection*{BIAS}
As mentioned above, we use biases when calculating the outputs. The nodes, with value $B$, are called the bias nodes, and the weights, $b$, are called the bias weights. But why do we need those? 

Suppose we have two inputs of value zero, and one output which should not be zero (this could for instance be a NOR gate). Without the bias, we will not be able to get any other output than zero, and in fact the network would struggle to find the right weights even if the output had been zero. 

The bias value $B$ does not really matter since the network will adjust the bias weights with respect to it, and is usually set to 1 and ignored in the calculations. [2]

\subsubsection*{Learning rate}
In principle, the weights could be updated without adding any learning rate ($\eta=1$), but this can in practice be problematic. It is easy to imagine that the outputs can be fluctuating around the targets without decreasing the error, which is not ideal, and a learning rate can be added to avoid this. The downside is that with a low learning rate the network needs more training to obtain the correct results (and it takes more time), so we need to find a balance. 

\subsubsection*{Cost function}\label{sec:cost_function}
The cost function is what defines the error, and in logistic regression the cross-entropy function is a naturally choice. [3] It reads
\begin{empheq}[box={\mybluebox[5pt]}]{equation}
c(\boldsymbol{W}) = -\sum_{i=1}^n\Big[y_i\log f(\boldsymbol{x}_i^T\boldsymbol{W})+(1-y_i)\log[1-f(\boldsymbol{x}_i^T\boldsymbol{W})]\Big]
\label{eq:cross_entropy}
\end{empheq}
where $\boldsymbol{W}$ contains all weights, included the bias weight ($\bb{W}\equiv[b,\bb{W}]$), and similarly does $\bb{x}$ include the bias node, which is 1; $\bb{x}\equiv[1,\bb{x}]$. Further, the $f(x)$ is the activation function discussed in next section.

The cross-entropy function is derived from likelyhood function, which famously reads
\begin{equation}
p(y|x)=\hat{y}^y\cdot(1-\hat{y})^{1-y}.
\end{equation}
Working in the log space, we can define a log likelyhood function
\begin{align}
\log\Big[p(y|x)\Big]&=\log\Big[\hat{y}^y\cdot(1-\hat{y})^{1-y}\Big]\\
&=y\log\hat{y}+(1-y)\log(1-\hat{y})
\end{align}
which gives the log of the probability of obtaining $y$ given $x$. We want this quantity to increase then the cost function is decreased, so we define our cost function as the negative log likelyhood function. [7]

Additionally, including a regularization parameter $\lambda$ inspired by Ridge regression is often convenient, such that the cost function is
\begin{equation}
c(\bb{W})^+=c(\bb{W})+\lambda||\bb{W}||_2^2.
\end{equation}
We will later study how this regularization affects the classification accuracy. 

\subsubsection*{Activation function}\label{sec:sigmoid1}
Above, we were talking about the activation function, which is used to activate the net output. In binary models, this is often just a step function firing when the net output is above a threshold. For continuous outputs, the logistic function given by
\begin{empheq}[box={\mybluebox[5pt]}]{equation}
f(x)=\frac{1}{1+e^{-x}}.
\label{eq:logistic}
\end{empheq}
is usually used in logistic regression to return a probability instead of a binary value. This function has a simple derivative, which is advantageous when calculating a large number of derivatives. As shown in section \ref{sec:sigmoid_der}, the derivative is simply
\begin{equation}
\frac{df(x)}{dx}=x(1-x).
\label{eq:logistic_der}
\end{equation}

$tanh(x)$ is another popular activation function in logistic regression, which more or less holds the same properties as the logistic function. 

\subsubsection*{Backward phase}
Now all the tools for finding the outputs are in place, and we can calculate the error. If the outputs are larger than the targets (which are the exact results), the weights need to be reduced, and if the error is large the weights need to be adjusted a lot. The weight adjustment can be done by any minimization method, and we will look at a couple of gradient methods. To illustrate the point, we will stick to the \textbf{gradient descent} (GD) method in the calculations, even though other methods will be used later. The principle of GD is easy: each weight is "moved" in the direction of steepest slope,
\begin{empheq}[box={\mybluebox[5pt]}]{equation}
\bb{w}^+= \bb{w} - \eta\cdot\frac{\partial c(\boldsymbol{w})}{\partial \bb{w}},
\label{eq:w_update}
\end{empheq}
where $\eta$ is the learning rate and $c(\bb{w})$ is the cost function. We use the chain rule to simplify the derivative
\begin{equation}
\frac{\partial c(\bb{w})}{\partial \bb{w}} =\frac{\partial c(\bb{w})}{\partial out} \cdot\frac{\partial out}{\partial net} \cdot\frac{\partial net}{\partial \bb{w}}
\end{equation}
where the first is the derivative of the cost function with respect to the output. For the cross-entropy function, this is
\begin{equation}
\frac{\partial c(\bb{w})}{\partial out}=-\frac{y}{out}+\frac{1-y}{1-out}.
\end{equation}
Further, the second derivative is the derivative of the activation function with respect to the output, which is given in \eqref{eq:logistic_der}
\begin{equation}
\frac{\partial out}{\partial net}=out(1-out).
\end{equation}
The latter derivative is the derivative of the net output with respect to the weights, which is simply
\begin{equation}
\frac{\partial net}{\partial \bb{w}}=\bb{x}.
\end{equation}

If we now recall that $out=f(\bb{x}^T\bb{w})$, we can write 
\begin{equation}
\frac{\partial c(\bb{w})}{\partial \bb{w}}=[f(\bb{x}^T\bb{w})-\bb{y}]\bb{x}
\end{equation}
and obtain a weight update algorithm
\begin{empheq}[box={\mybluebox[5pt]}]{align}
\bb{w}^+= \bb{w} - \eta\cdot[f(\bb{x}^T\bb{w})-\bb{y}]^T\bb{x}.
\end{empheq}
where the bias weight is included implicitly in $\bb{w}$ and the same applies for $\bb{x}$.

\subsection{Feed-forward Neural Networks (FNN)} \label{sec:neural_network}
If you have understood logistic regression, understanding a neural network should not be a difficult task. According to  \textbf{the universal approximation theorem}, a neural network with only one hidden layer can approximate any continuous function. [8] However, often multiple layers are used since this tends to give fewer nodes in total. 

In figure \eqref{fig:neural_network}, a two-layer neural network (one hidden layer) is illustrated. It has some similarities with the logistic regression model in figure \eqref{fig:single_perceptron}, but a hidden layer and multiple outputs are added. In addition, the output is no longer probabilities and can take any number, which means that we do not need to use the logistic function on the outputs anymore.

\begin{figure} [H]
	\centering
	\begin{tikzpicture}
	
	% Define outputs
	\node[] (center) {};
	\node[input, above=0.3em of center] (o1) {$o_1$};
	\node[input, below=0.3em of center] (o2) {$o_2$};
	
	% Draw lines from output nodes
	\node[right of=o1] (righto1) {};
	\node[right of=o2] (righto2) {};
	\path[draw,->] (o1) -- (righto1);
	\path[draw,->] (o2) -- (righto2);
	
	% Hidden nodes
	\node[input,left=5em of center] (h3) {$h_3$};
	\node[input,above of=h3] (h2) {$h_2$};
	\node[input,above of=h2] (h1) {$h_1$};
	\node[input,below of=h3] (h4) {$h_4$};
	\node[input,below of=h4] (h5) {$h_5$};
	\node[input,above of=h1] (b2) {$b_2$};
	
	% Draw lines from hidden nodes
	\path[draw,->] (h1) -- (o1);
	\path[draw,->] (h2) -- (o1);
	\path[draw,->] (h3) -- (o1);
	\path[draw,->] (h4) -- (o1);
	\path[draw,->] (h5) -- (o1);
	\path[draw,->] (b2) -- (o1);
	
	\path[draw,->] (h1) -- (o2);
	\path[draw,->] (h2) -- (o2);
	\path[draw,->] (h3) -- (o2);
	\path[draw,->] (h4) -- (o2);
	\path[draw,->] (h5) -- (o2);
	\path[draw,->] (b2) -- (o2);
	
	% Define place left of left
	\node[input,left=5em of h3] (x2) {$x_2$};
	\node[input,above of=x2] (x1) {$x_1$};
	\node[input,below of=x2] (x3) {$x_3$};
	\node[input,above of=x1] (b1) {$b_1$};
	
	% Draw lines from input nodes
	\path[draw,->] (x1) -- (h1);
	\path[draw,->] (x1) -- (h2);
	\path[draw,->] (x1) -- (h3);
	\path[draw,->] (x1) -- (h4);
	\path[draw,->] (x1) -- (h5);
	
	\path[draw,->] (x2) -- (h1);
	\path[draw,->] (x2) -- (h2);
	\path[draw,->] (x2) -- (h3);
	\path[draw,->] (x2) -- (h4);
	\path[draw,->] (x2) -- (h5);
	
	\path[draw,->] (x3) -- (h1);
	\path[draw,->] (x3) -- (h2);
	\path[draw,->] (x3) -- (h3);
	\path[draw,->] (x3) -- (h4);
	\path[draw,->] (x3) -- (h5);
	
	\path[draw,->] (b1) -- (h1);
	\path[draw,->] (b1) -- (h2);
	\path[draw,->] (b1) -- (h3);
	\path[draw,->] (b1) -- (h4);
	\path[draw,->] (b1) -- (h5);
	
	% Draw lines towards input nodes
	\node[left of=x1] (leftx1) {};
	\node[left of=x2] (leftx2) {};
	\node[left of=x3] (leftx3) {};
	\path[draw,->] (leftx1) -- (x1);
	\path[draw,->] (leftx2) -- (x2);
	\path[draw,->] (leftx3) -- (x3);
	
	% Add some text
	\node[below=5.1em of x2,font=\scriptsize] {input};
	\node[below=5em of h3,font=\scriptsize] {hidden};
	\node[below=5.8em of center,font=\scriptsize] {output};
	\end{tikzpicture}
	\caption{Neural network with 3 input nodes, 5 hidden nodes and 2 output nodes, in addition to the bias nodes.}
	\label{fig:neural_network}
\end{figure}

Without a hidden layer, we have seen that the update of weights is quite straight forward. For a neural network consisting of multiple layers, the question is: how do we update the weights when we do not know the values of the hidden nodes? And how do we know which layer causing the error? This will be explained in section \ref{sec:backward}, where one of the most popular techniques for that is discussed. Before that we will generalize the forward phase presented in logistic regression.

Since it will be quite a lot calculations, I will just express the results here, and move the calculations to Appendix B. The forward phase in a three-layer perceptron is
\begin{empheq}[box={\mybluebox[5pt]}]{align}
net_{hi}&=\sum_jw_{ji}^{(1)}\cdot x_j\notag\\
out_{hi}&=\text{f}(net_{hi})\notag\\
\notag\\
net_{ki}&=\sum_jw_{ji}^{(2)}\cdot out_{hj}\\
out_{ki}&=\text{f}(net_{ki})\notag\\
\notag\\
net_{oi}&=\sum_jw_{ji}^{(3)}\cdot out_{kj}\notag\\
out_{oi}&=\text{f}(net_{oi})\notag
\end{empheq}
which can easily be turned into vector form. The backward propagation follows from the two-layer example, and we get
\begin{empheq}[box={\mybluebox[5pt]}]{align}
w_{ij}^{(3)}&=w_{ij}^{(3)}-\eta\cdot\delta_{oj}\cdot out_{ki}\notag\\
\notag\\
w_{ij}^{(2)}&=w_{ij}^{(2)}-\eta\sum_{k=1}^O\delta_{ok}\cdot w_{jk}^{(3)}\cdot f'(out_{kj})\cdot out_{hi}\notag\\
\notag\\
w_{ij}^{(1)}&=w_{ij}^{(1)}-\eta\sum_{k=1}^O\sum_{l=1}^K\delta_{ok}\cdot w_{lk}^{(3)}\cdot f'(out_{kl})\cdot w_{jl}^{(2)}f'(out_{hj})\cdot x_i\notag
\end{empheq}
where we again use the short hand 
\begin{equation*}
\delta_{oj}=(t_j-out_{oj})\cdot f'(out_{oj}).
\end{equation*}
If we compare with the weight update formulas for the two-layer case, we recognize some obvious connections, and it is easy to imagine how we can construct a general weight update algorithm, no matter how many layers we have.

\subsection{Convolutional Neural Networks (CNN)}
Convolutional neural networks are known to be good at image classification, but how can we use it on sound classification? The idea is to turn the samples into images, and for that one can for example use a mel spectrogram, as described in the theory section. 

CNNs are based on FNNs, but the image is initially feed through a few layers that extract the most important information. 

\subsubsection*{Convolutional layer}
Initially, the image is sent into a convolutional layer, which is meant to reveal structures and shapes in the image. The way we do it, is to introduce a filter that we slide over the entire image and multiply with all pixels (with overlap). Every time the filter is multiplied with a set of pixels, we sum all the multiplications and add the value to an activation map. The activation map is completed after we have multiplied the filter with the entire image. A typical filter has dimensions 16x16, but depends on the image shape. It is important to choose a filter that is big enough to cover structures. 

\subsubsection*{Pooling layer}
It is common to insert a pooling layer in-between convolutional layers, but why do we do that? A pooling layer is just a way to reduce the dimensionality of the representation such that we do not need to optimize that many parameters, but also helps to control overfitting. It works the way that we divide the representation (usually an activation map) into regions of equal size and represent each region with one single number. Max pooling is apparently the most popular technique, which just represents each region with the largest number in that region, but it is also possible to use average pooling, min pooling etc.. [\url{http://cs231n.github.io/convolutional-networks/}]

As an example, we can divide the image into 2x2 regions, which will reduce the size of the representation with 75\%. 

\subsubsection*{Dropout}
Dropout is widely used in neural network layers, and is another method to prevent overfitting. The way it works is just to drop out units in the current layer.

To understand the idea, we need to take a close look at why overfitting occurs. Overfitting occurs when neighbor nodes collaborate, and become a powerful cluster which fits the model too specifically for the training set. To brake up those evil clusters, we can simply set random nodes to zero. [\url{https://medium.com/@amarbudhiraja/https-medium-com-amarbudhiraja-learning-less-to-learn-better-dropout-in-deep-machine-learning-74334da4bfc5}]

\subsubsection*{Fully connected layer}
The last part of a convolutional network is often refereed to as a fully connected layer, which is just a FNN. To be able to feed this layer with a input, we need to flatten out the reference. 

\subsection{Recurrent Neural Networks (RNN)}
Last, but not least, we will examine recurrent neural networks. In the time domain, the sound at one time is dependent on sounds at other times, but the methods above are not able to utilize that because they lack memory. 

In RNNs, information is shared between the different nodes, in that sense they have a short memory. This has 

\subsubsection{Long Short-Term Memory (LSTM)}


\subsubsection{Gated Recurrent Unit (GRU)}

