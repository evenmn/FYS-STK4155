\section{Methods} \label{sec:methods}

\subsection{Resampling techniques} \label{sec:resampling}
A resampling technique is..
There are plenty of resampling techniques, and we have already went through several of them in this course:
\begin{itemize}
\item{Validation set approaches}
\item{Leave one out validation}
\item{Jackknife resampling}
\item{K-fold validation}
\item{Bootstrap method}
\item{Blocking method}.
\end{itemize}
For this particular project we have been focusing on the bootstrap and the k-fold validation methods, so here I will cover them only

\subsubsection{Bootstrap method} \label{sec:bootstrap}

\subsubsection{K-fold validation method} \label{sec:kfold}


\subsection{Singular Value Decomposition (SVD)} \label{sec:svd}

\subsection{Minimization methods} \label{sec:minimization}
When the interaction term is excluded, we know which $\alpha$ that corresponds to the energy minimum, and it is in principle no need to try different $\alpha$'s. However, sometimes we have no idea where to search for the minimum point, and we need to try various $\alpha$ values to determine the lowest energy. If we do not know where to start searching, this can be a time consuming activity. Would it not be nice if the program could do this for us?

In fact there are multiple techniques for doing this, where the most complicated ones obviously also are the best. Anyway, in this project we will have good initial guesses, and are therefore not in need for the most fancy algorithms. 

\subsubsection{Gradient Descent} \label{sec:gd}
Perhaps the simplest and most intuitive method for finding the minimum is the gradient descent method (GD), which reads
\begin{equation}
\label{eq:GD}
\beta_i^{\text{new}}=\beta_i - \eta\cdot\frac{\partial Q(\beta_i)}{\partial\beta_i}
\end{equation}
where $\beta_i^{\text{new}}$ is the updated $\beta$ and $\eta$ is a step size, in machine learning often refered to as the learning rate. The idea is to find the gradient of the cost function $Q(\vec{\beta})$ with respect to a certain $\beta_i$, and move in the direction which minimizes the cost function. This is repeated until a minimum is found, defined by either
\begin{equation}
\frac{\partial Q(\beta_i)}{\partial\beta_i}<\varepsilon
\end{equation}
or that the change in $\beta_i$ for the past $x$ steps is small. 
\par 
\vspace{3mm}

Before we can implement equation \eqref{eq:GD}, we need an expression for the derivative of $Q$ with respect to $\beta_i$. The general form of the cost function as discussed in section \ref{sec:general} reads
\begin{equation}
Q(\vec{\beta},\lambda,q)=\sum_{i=1}^{n}\Big(y_i-\beta_0-\sum_{j=1}^px_{ij}\beta_j\Big)^2+\lambda\sum_{j=1}^p\beta_j^q,
\label{eq:cost_gen}
\end{equation}
and its derivative with respect to $\beta_k$ is
\begin{equation}
\frac{\partial Q(\vec{\beta},\lambda,q)}{\partial\beta_k}=-2\sum_i^n\Big(y_i-\beta_0-\sum_{j=1}^px_{ij}\beta_j\Big)x_{ik}+q\lambda\beta_k^{q-1}.
\label{eq:der_cost_gen}
\end{equation}
The vectorized version looks like
\begin{equation}
\frac{\partial Q(\vec{\beta},\lambda,q)}{\partial\vec{\beta}}=-2\hat{X}^T(\vec{y}-\hat{X}\vec{\beta})+q\lambda\vec{\beta}^{q-1}
\label{eq:der_cost_gen_vec}
\end{equation}

The algorithm of this minimization method is thus as follows:

\lstset{basicstyle=\scriptsize}
\begin{lstlisting}

initialize beta
for(max number of iterations with minimizing)
	
	calculate gradient
	
	Check if dQ/dbeta < eps or beta fluctuation over the last 5 steps is < eps
	
	
		if yes, break loop
		if no, update beta such that Q is minimized


\end{lstlisting}