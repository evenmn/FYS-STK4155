\section{Methods} \label{sec:methods}

\subsection{Resampling techniques} \label{sec:resampling}
A resampling technique is a way of estimating the variance of data sets without calculating the covariance. As we saw in section section \ref{sec:error_analysis}, the true covariance is given by a double loop which we will avoid calculating if possible. There are different ways of doing this, and we have already went through several of them in this course:
\begin{itemize}
\item{Jackknife resampling}
\item{K-fold validation}
\item{Bootstrap method}
\item{Blocking method}.
\end{itemize}

For this particular project we have been focusing on the bootstrap and the K-fold validation methods, so only they will be covered here.

\subsubsection{Bootstrap method} \label{sec:bootstrap}
When we construct a data set, we usually draw samples from a probability density function (PDF) and get a set of samples $\vec{x}$. If we draw a large number of samples, the sample variance will approach the variance of the PDF. The bootstrap method turns this upside down, and tries to estimate the PDF given a set data set, because if we know the PDF, we know in principle everything about the data set. 

The assumption we need to make, is that the relative frequency of $x_i$ equals $p(x_i)$, which is reasonable (for instance, think about how the histogram looks like when we draw samples from a normal distribution). In this project the vector that we want to find, $\vec{\beta}(x,y)$, is a function of two set of variables. Fortunately, they are independent of eachother, so we can safely apply the independent bootstrap on them separately. The independent bootstraps goes as
\begin{enumerate}
\item Draw $n$ samplings from the data set $\vec{x}$ with replacement and denote the new data set as $\vec{x}^*=\{x_1,x_2,\hdots,x_n\}$
\item Compute $\vec{\beta}(\vec{x}^*)\equiv\vec{\beta}^*$
\item Repeat the procedure above $K$ times
\item The average value of all $K$ $\vec{\beta}^*$'s are stored in a new vector $\vec{\bar{\beta}}^*$
\item Finally, the variance of $\vec{\bar{\beta}}^*$ should correspond to the sample variance
\end{enumerate}
\cite{BootstrapEfron}

The implementation could look something like this
\lstset{basicstyle=\scriptsize}
\begin{lstlisting}
def bootstrap(data, K=1000):
    dataVec = np.zeros(K)
    for k in range(K):
        dataVec[k] = np.average(np.random.choice(data, len(data)))
    Avg = np.average(dataVec)
    Var = np.var(dataVec)
    Std = np.std(dataVec)
    
    return Avg, Var, Std
\end{lstlisting}
which is strongly inspired by the lecture notes [NEED REFERENCE]

It is also worth to mention that we should hold back a small part of the original data set for testing, since we need to test the model on data that it has not seen before.

\subsubsection{K-fold validation method} \label{sec:kfold}
K-fold validation is a method that has more describing name than many of its fellow methods. The idea is to make the most use of our data by splitting it into $k$ folds and training $k$ times on it. Every time we train, we leave out one of the folds, which gonna be our validation data. This validation data needs to be different every time, and we are therefore restricted to $k$ unique training sessions. A typical overview looks like this:
\begin{itemize}
	\item Split data set into $k$ equally sized folds
	\item Use the $k-1$ first folds as training data, and leave the $k$'th fold for validation
	\item Use the $k-2$ first folds plus the $k$-th fold as training data, and leave the $(k-1)$'th fold for validation
	\item Continue until all folds are used as training data
\end{itemize}

\cite{Berkeley}

\subsection{Minimization methods} \label{sec:minimization}
Suppose we have a very simple model trying to fit a straight line to data points. In that case, we could manually vary the coefficients and find a line that fits the points quite good. However, when the model gets more complicated, this can be a time consuming activity. Would it not be good if the program could do this for us?

In fact there are multiple techniques for doing this, where the most complicated ones obviously also are the best. Anyway, in this project we will have good initial guesses, and are therefore not in need for the most fancy algorithms. We will stick to gradient descent in this project, which might be the simplest method for our purposes.

\subsubsection{Gradient Descent} \label{sec:gd}
Perhaps the simplest and most intuitive method for finding the minimum is the gradient descent method (GD), which reads
\begin{equation}
\label{eq:GD}
\beta_i^{\text{new}}=\beta_i - \eta\cdot\frac{\partial Q(\beta_i)}{\partial\beta_i}
\end{equation}
where $\beta_i^{\text{new}}$ is the updated $\beta$ and $\eta$ is a step size, in machine learning often refered to as the learning rate. The idea is to find the gradient of the cost function $Q(\vec{\beta})$ with respect to a certain $\beta_i$, and move in the direction which minimizes the cost function. This is repeated until a minimum is found, defined by either
\begin{equation}
\frac{\partial Q(\beta_i)}{\partial\beta_i}<\varepsilon
\end{equation}
or that the change in $\beta_i$ for the past $x$ steps is small. 
\par 
\vspace{3mm}

Before we can implement equation \eqref{eq:GD}, we need an expression for the derivative of $Q$ with respect to $\beta_i$. The general form of the cost function, as discussed in section \ref{sec:general}, reads
\begin{equation}
Q(\vec{\beta},\lambda,q)=\sum_{i=1}^{n}\Big(y_i-\beta_0-\sum_{j=1}^px_{ij}\beta_j\Big)^2+\lambda\sum_{j=1}^p|\beta_j|^q,
\label{eq:cost_gen}
\end{equation}
and its derivative with respect to $\beta_k$ is
\begin{equation}
\frac{\partial Q(\vec{\beta},\lambda,q)}{\partial\beta_k}=-2\sum_i^n\Big(y_i-\beta_0-\sum_{j=1}^px_{ij}\beta_j\Big)x_{ik}+\frac{\beta_k}{|\beta_k|}q\lambda|\beta_k|^{q-1}.
\label{eq:der_cost_gen}
\end{equation}
The vectorized version looks like
\begin{equation}
\frac{\partial Q(\vec{\beta},\lambda,q)}{\partial\vec{\beta}}=-2\hat{X}^T(\vec{y}-\hat{X}\vec{\beta})+\frac{\vec{\beta}}{|\vec{\beta}|}q\lambda|\vec{\beta}|^{q-1}
\label{eq:der_cost_gen_vec}
\end{equation}
where all operations are element wise. The algorithm of this minimization method is thus as follows:

\lstset{basicstyle=\scriptsize}
\begin{lstlisting}
while dbeta > epsilon:
	e = z - X.dot(beta)
    debeta = 2*X.T.dot(e) -np.sign(beta)*q*lambda*np.power(abs(beta), q-1)
    beta += eta*dbeta
\end{lstlisting}