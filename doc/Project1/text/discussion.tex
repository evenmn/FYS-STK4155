\section{Discussion} \label{sec:discussion}
The first thing we can see from the results, is that the fitted functions in figure \eqref{fig:franke_plots} look very similar with and without noise. This is probably caused by too small noise. However, we observe that both the OLS regression and Ridge are really good fits compared to the actual function in figure \eqref{fig:data}. Lasso is also similar, but with the naked eye we can see that it is obviously not as good as OLS and Ridge. 

This is also reflected in the error analysis, where the MSE is way larger for Lasso compared to OLS and Ridge. OLS seems to have a slightly lower MSE than Ridge again, but for Ridge with gradient descent the MSE is even higher than for Lasso. From this, it may seems like the the gradient descent method does not work properly, because the cost function is obviously not fully minimized. When it comes to the R$^2$-score, we observe that also here OLS got the best score barely in front of Ridge, with RidgeGD and Lasso far behind. An interesting aspect is that the self-implemented Lasso function gives better results than the Scikit Learn Lasso function.

We expect the MSE produced by K-fold validation method on test data to be larger than the true MSE, and the R$^2$ score to be lower. We can see that this is the case here, and it is a good thing that it does not differ too much. A large difference indicated a poor training session. 

Furthermore, from the R$^2$ vs. $\lambda$ plot in figure \eqref{fig:R2_scores}, we can see that Ridge mostly got a better R$^2$-score than Lasso, only beaten around $\lambda=1$. What surprises me, is that the methods do not get more similar when the penalty decreases, since they should both approach OLS when the penalty gets smaller. In the second plot in the same figure, we again find Lasso to give the lowest R$^2$-score. 

In figure \eqref{fig:beta_plots}, we can see that the beta values produced by Scikit's OLS function and the self-built OLS-function are more or less identical. The same applies to Ridge, so we can conclude that those two methods are implemented correctly (also based on the error analysis). The situation is unlike for the gradient descent methods, where the coefficients differ. Especially for RidgeGD, the difference is significantly both in sign and magnitude. For Lasso, we can reveal some similarities, and at least the magnitudes are similar. Again it is tempting to conclude that the minimization is wrong. Something I find strange, is that every second coefficient is negative and every second is positive for OLS, and obtain a cool pattern which I cannot explain. 

We have earlier stated that we expect the coefficients in Ridge regression to be smaller than the coefficients in OLS regression, and the coefficients in Lasso regression should be even smaller than Ridge again. This is exactly what we see in figure \eqref{fig:beta_plots}.
\vspace{1cm}

Now over to the real terrain data. In figure \eqref{fig:terrain_plots}, we can see that OLS reproduces the sharp peak of the volcano best among the methods, followed by Ridge and finally Lasso. This is expected from regression on the Franke function. Another thing we observe, is that none of the models is able to reproduce the flat ocean around the volcano. This is because we have limited the models to 5 degrees in each direction, and that is not enough degrees to fit those areas. 

Again we observe that the self-built OLS function and Scikit Learn's OLS function give identical MSE and R$^2$-scores, and exactly the same applies for Ridge. When it comes to Lasso, the self-built function again gives a lower MSE and higher R$^2$-score than the function of Scikit learn, which is a bit surprising. Ridge produced from gradient descent is still weak compared to all the other methods. The K-fold validation MSE is in fact lower than the true MSE, but the difference is small, so the training session seems to have been performed correctly. 

When it comes to the how the R$^2$-score is varying as a function of the penalty, we can see that also here the R$^2$-score from Lasso crosses the R$^2$-score from Ridge. This happens around $\lambda=0.1$, and that is the only point where Lasso can be considered as better than Ridge. For small penalties, Ridge is way better than Lasso.