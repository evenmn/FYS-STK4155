\section{Code} \label{sec:code}
The code is mainly implemented in Python, due to its neatness and flexibility. The numpy package has a lot of functions which are both fast and convenient for machine learning purposes, and there exist some packages that provide easy and fast machine learning tools such as Scikit-Learn, Keras and Tensorflow. However, the neural networks that we implement from scratch will not be as fast in Python as in a low-level language, and they where therefore implemented in C++ as well. 

\subsection{Code structure} \label{sec:structure}
In order to reuse the code where possible, we ended up with several functions that communicate in criss-cross. The main functions are called \textit{find\_energy.py} and \textit{classifier.py}, where the former estimates the energy of a Ising lattice using linear regression and neural network, getting the data from \textit{Ising\_1D.py}. The latter estimates the phase of the Ising model using logistic regression and neural networks, getting the data from \textit{Ising\_2D.py} (taken from [3]). \textit{error\_tools.py} consists of functions that returns the error, including MSE, R2 and Accuracy, which communicates with \textit{resampling.py} as well. Both main functions are largely dependent on \textit{neural\_network.py}, which gets the optimization functions from \textit{optimization.py} and the activation functions from \textit{activation.py}.
\begin{figure} [H]
\centering
\begin{tikzpicture}[auto,node distance=0.9cm]

  % Create main nodes
  \node[entity] (node1) {find energy};
  \node[entity] (rel1) [below right = of node1] {linear regression};
  \node[entity] (node2) [above right = of rel1] {classifier};
  \node[entity] (rel2) [below = of rel1, level distance=0.1cm] {Error tools};
  \node[attribute] (att1) [below left = of node1] {Ising 1D};
  \node[attribute] (att2) [below right = of node2] {Ising 2D};
  \node[entity] (rel3) [below = of rel2] {Resampling};
  \node[relationship] (rel4) [below = of rel3] {Neural Network};
  \node[attribute] (att3) [right = of rel4] {Optimization};
  \node[attribute] (att4) [left = of rel4] {Activation};
  
  
  % Draw paths
  \path (node1) edge node {} (att1);
  \path (node2) edge node {} (att2);
  \path (node1) edge node {} (rel1);
  \path (node1) edge node {} (rel2);
  \path (node1) edge node {} (rel3);
  \path (node1) edge node {} (rel4);
  \path (node2) edge node {} (rel2);
  \path (node2) edge node {} (rel4);
  \path (rel4) edge node {} (att3);
  \path (rel4) edge node {} (att4);
  \path (rel4) edge node {} (rel3);
  \path (rel3) edge node {} (rel2);

\end{tikzpicture}
\caption{Code structure}
\label{fig:codestructure}
\end{figure}

\subsection{Algorithm}
The actual algorithm consists of the results above, and we need to order them correctly. To increase the performance and the neatness, I will present a vectorized implementation (i.e. use linear algebra to solve it instead of sum over piecewise elements). For the forward propagation (when we get the outputs), it is easy to see that it can be vectorized,
\begin{equation}
net_j = \sum_i (w_{ij}\cdot X_i + b_i)\quad\Rightarrow\quad net = W\cdot X + b.
\end{equation}
where $net$ is a vector. Also the backward propagation (when we update the weights) can be vectorized. Firstly the differentiation of the error function can be vectorized:
\begin{equation}
\frac{\partial E_{TOT}}{\partial out_i}=-(t_i-out_i)\quad\Rightarrow\quad \frac{\partial E_{TOT}}{\partial out}=-(t-out)
\end{equation}
where we have vectors on both sides. Those vectors will have the same length as $out(1-out)$, such that we can define a quantity $\delta^o$ and apply the Hadamard multiplication:
\begin{equation}
\delta^o = \frac{\partial E_{TOT}}{\partial out}\odot\frac{\partial\text{sigmoid}(out)}{\partial out}=-(t-out)\odot out(1-out)
\end{equation}
which will be used in both the weight update and the bias update. To fulfil the weight update, we need to take the outer product of $\delta^o$ with the inputs $X_i$, and we then have $\partial E_{TOT}/\partial W$, which of course has the same dimensions as the weight matrix. 
\begin{equation}
W^+ = W - \eta(\delta^o\otimes X)
\end{equation}
\begin{equation}
b^+ = b - \eta\cdot\delta^o
\end{equation}

It should look something like this:
\begin{itemize}
	\item \textbf{Declare all variables, define inputs, targets and quantities, and initialize the weights}\\
	\#Iterations = ...\\
	Eta = ...\\
	\\
	X = [[...] ... [...]]\\
	t = [[...] ... [...]]\\
	\\
	W = 2*random(0,1) - 1\\
	b = 2*random(0,1) - 1\\
	
	\item \textbf{Training - calculate outputs for all samples and then update weights. Repeat \#Iterations times}\\
	for i < \#Iterations\\
	for j < \#Samples\\
	net = X*W + b\\
	out = sigmoid(net)\\
	\\
	W = W + eta*(t - out)*out(1-out)*X\\
	b = b + eta*(t - out)*out(1-out)\\
	
	\item \textbf{Recall - Use the weight to find answer to problems with unknown answers}\\
	X = [[...] ... [...]]		Inputs with unknown outputs\\
	net = X*W + b\\
	ans = sigmoid(out)\\
\end{itemize}

\subsection{Implementation}
For the algorithms above, there exist huge possibilities for vectorization, and basically all the loops can be avoided using vectorization provided by numpy. 

\lstset{basicstyle=\scriptsize}
\begin{lstlisting}
def linear(X, t, T, eta = 0.1):

I = len(X[0])
O = len(t[0])
M = len(X)

W = 2*np.random.random([I, O]) - 1
b = 2*np.random.random(O) - 1

for iter in range(T):
for i in range(M):
net = np.dot(X[i], W) + b
out = sigmoid(net)

deltao = -(t[i] - out)*sig_der(out)
W = W - eta * np.outer(np.transpose(X[i]), deltao)
b = b - eta * deltao
return W, b

\end{lstlisting}
where sigmoid is the sigmoid function used in the weight calculations above and sig\_der is its derivative. To get this working, the input array $X$ and the target array $t$ need to be numpy arrays, and a call to the function can be done like this
\begin{lstlisting}
X = np.array([[0,0], [0,1], [1,0], [1,1]])
t = np.array([[0], [1], [1], [1]])

W, b = linear(X, t, 1000)
\end{lstlisting}
which can be recognized as an OR gate. The logic functions are popular examples when it comes to neural network, because (1) they are easy to work with (2) they display the constraints of a single perceptron in a easy way. In next section we are going to discuss this, and why we need multi-layer perceptrons for certain logic functions. However, back to the implementation. Since we now know the weights, we can use them to recall, which is simply done by
\begin{lstlisting}
X_ = np.array([0,1])
net = np.dot(X_, W) + b
out = sigmoid(net)
print(out)
>>> 1
\end{lstlisting}
which should give 1 according to the OR gate. We could train this network to understand NOR, AND and NAND gates as well, but what happens if you try to teach it the XOR gate? 

For complete programs, please visit \\ \url{https://github.com/evenmn/Machine-Learning}.