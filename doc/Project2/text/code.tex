\section{Code} \label{sec:code}

\subsection{Code structure} \label{sec:structure}

\begin{figure}
\centering
\begin{tikzpicture}[auto,node distance=0.9cm]

  % Create entity node
  \node[entity] (node1) {fit\_terrain.py};
  
  % Establish relationships
  \node[relationship] (rel1) [below right = of node1] {regression\_2D.py};
  \node[relationship] (rel2) [below left = of node1] {resampling.py};
  
  % Create another entity node
  \node[entity] (node2) [below left = of rel1] {fit\_franke.py} child[grow=down,level distance=1.5cm] {node[attribute] {franke.py}};
  
  % Draw paths
  \path (rel1) edge node {} (node1);
  \path (rel2) edge node {} (node1);
  \path (rel1) edge node {} (node2);
  \path (rel2) edge node {} (node2);

\end{tikzpicture}
\caption{Code structure}
\label{fig:codestructure}
\end{figure}

\subsection{Algorithm}
The actual algorithm consists of the results above, and we need to order them correctly. To increase the performance and the neatness, I will present a vectorized implementation (i.e. use linear algebra to solve it instead of sum over piecewise elements). For the forward propagation (when we get the outputs), it is easy to see that it can be vectorized,
\begin{equation}
net_j = \sum_i (w_{ij}\cdot X_i + b_i)\quad\Rightarrow\quad net = W\cdot X + b.
\end{equation}
where $net$ is a vector. Also the backward propagation (when we update the weights) can be vectorized. Firstly the differentiation of the error function can be vectorized:
\begin{equation}
\frac{\partial E_{TOT}}{\partial out_i}=-(t_i-out_i)\quad\Rightarrow\quad \frac{\partial E_{TOT}}{\partial out}=-(t-out)
\end{equation}
where we have vectors on both sides. Those vectors will have the same length as $out(1-out)$, such that we can define a quantity $\delta^o$ and apply the Hadamard multiplication:
\begin{equation}
\delta^o = \frac{\partial E_{TOT}}{\partial out}\odot\frac{\partial\text{sigmoid}(out)}{\partial out}=-(t-out)\odot out(1-out)
\end{equation}
which will be used in both the weight update and the bias update. To fulfil the weight update, we need to take the outer product of $\delta^o$ with the inputs $X_i$, and we then have $\partial E_{TOT}/\partial W$, which of course has the same dimensions as the weight matrix. 
\begin{equation}
W^+ = W - \eta(\delta^o\otimes X)
\end{equation}
\begin{equation}
b^+ = b - \eta\cdot\delta^o
\end{equation}

It should look something like this:
\begin{itemize}
	\item \textbf{Declare all variables, define inputs, targets and quantities, and initialize the weights}\\
	\#Iterations = ...\\
	Eta = ...\\
	\\
	X = [[...] ... [...]]\\
	t = [[...] ... [...]]\\
	\\
	W = 2*random(0,1) - 1\\
	b = 2*random(0,1) - 1\\
	
	\item \textbf{Training - calculate outputs for all samples and then update weights. Repeat \#Iterations times}\\
	for i < \#Iterations\\
	for j < \#Samples\\
	net = X*W + b\\
	out = sigmoid(net)\\
	\\
	W = W + eta*(t - out)*out(1-out)*X\\
	b = b + eta*(t - out)*out(1-out)\\
	
	\item \textbf{Recall - Use the weight to find answer to problems with unknown answers}\\
	X = [[...] ... [...]]		Inputs with unknown outputs\\
	net = X*W + b\\
	ans = sigmoid(out)\\
\end{itemize}

\subsection{Implementation}
For simplicity all the example codes will be Python scripts, and the numpy package will be used diligently because of its linear algebra features and its performance. Similarly one can use armadillo in C++ and if you wonder how to do the implementations in there, you could visit my github repository. Anyway, the best way to implement the algorithm is with a function which takes the parameters as arguments and returns the weights. We define all quantities as numpy arrays, and use \lstinline!np.dot! for the inner product, \lstinline!np.outer! for the outer product and simply multiply two arrays for the Hadamard product. If you do not trust this latter statement, \lstinline!np.multiply! should be corresponding. The implementation looks like this;

\lstset{basicstyle=\scriptsize}
\begin{lstlisting}
def linear(X, t, T, eta = 0.1):

I = len(X[0])
O = len(t[0])
M = len(X)

W = 2*np.random.random([I, O]) - 1
b = 2*np.random.random(O) - 1

for iter in range(T):
for i in range(M):
net = np.dot(X[i], W) + b
out = sigmoid(net)

deltao = -(t[i] - out)*sig_der(out)
W = W - eta * np.outer(np.transpose(X[i]), deltao)
b = b - eta * deltao
return W, b

\end{lstlisting}
where sigmoid is the sigmoid function used in the weight calculations above and sig\_der is its derivative. To get this working, the input array $X$ and the target array $t$ need to be numpy arrays, and a call to the function can be done like this
\begin{lstlisting}
X = np.array([[0,0], [0,1], [1,0], [1,1]])
t = np.array([[0], [1], [1], [1]])

W, b = linear(X, t, 1000)
\end{lstlisting}
which can be recognized as an OR gate. The logic functions are popular examples when it comes to neural network, because (1) they are easy to work with (2) they display the constraints of a single perceptron in a easy way. In next section we are going to discuss this, and why we need multi-layer perceptrons for certain logic functions. However, back to the implementation. Since we now know the weights, we can use them to recall, which is simply done by
\begin{lstlisting}
X_ = np.array([0,1])
net = np.dot(X_, W) + b
out = sigmoid(net)
print(out)
>>> 1
\end{lstlisting}
which should give 1 according to the OR gate. We could train this network to understand NOR, AND and NAND gates as well, but what happens if you try to teach it the XOR gate? 

For complete programs, please visit \\ \url{https://github.com/evenmn/Machine-Learning}.