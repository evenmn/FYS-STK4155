\section{Appendix B - Backward propagation} \label{sec:appendixb}

\subsection{Single layer}
Since the energy is not directly dependent on $w_{ij}$, we need to do a change of variables such that we get derivatives that we can calculate. Probably the best choice is
\begin{equation}
\frac{\partial E_{TOT}}{\partial w_{ij}}=\frac{\partial E_{TOT}}{\partial out_{j}}\cdot\frac{\partial out_{j}}{\partial net_{j}}\cdot\frac{\partial net_{j}}{\partial w_{ij}}.
\end{equation}
Standard differentiating of the error function from section \ref{sec:error_function} gives
\begin{equation}
\frac{\partial E_{TOT}}{\partial out_{j}}=-(t_j-out_j)
\end{equation}
which is a neat expression as pointed out above. To obtain the second fraction, we need to decide which sigmoid function to use. In the calculations I will use the first mentioned in section \ref{sec:sigmoid1}, that reads
\begin{equation}
out_j=\frac{1}{1+\exp(-net_j)}.
\end{equation}
and we have already found that
\begin{equation}
\frac{\partial out_{j}}{\partial net_{j}}=out_j(1-out_j).
\end{equation}
Furthermore the netto output is given by equation (\ref{eq:forward}),
\begin{equation}
net_j = \sum_{i=1}^{I} X_i\cdot w_{ij} + b_j\cdot 1,
\end{equation}
which simply gives
\begin{equation}
\frac{\partial net_{j}}{\partial w_{ij}}=X_i.
\end{equation}
We end up with 
\begin{equation}
\frac{\partial E_{TOT}}{\partial w_{ij}}=-(t_j-out_j)\cdot out_j(1-out_j)\cdot X_i
\end{equation}
and the weight updating formula
\begin{empheq}[box={\mybluebox[5pt]}]{equation}
w_{ij}^{+}=w_{ij} + \eta\cdot(t_j-out_j)\cdot out_j(1-out_j)\cdot X_i
\end{empheq}
Similarly we need to update the bias weights, with a formula similar to that one in equation (\ref{eq:w_update}):
\begin{equation}
b_i^+=b_i - \eta\cdot\frac{\partial E_{TOT}}{\partial b_i}.
\end{equation}
We do the same change of variables and the only thing that changes is the last part, $\partial net_i/\partial b_i$, which can easily be found to be 1. Then
\begin{empheq}[box={\mybluebox[5pt]}]{equation}
b_i^+=b_i + \eta\cdot(t_i-out_i)\cdot out_i(1-out_i)
\end{empheq}
and all the basics for the single perceptron are set. 

\subsection{Two-layer}
Backward propagation is probably the most used technique to update the weights, and is actually based on equation (\ref{eq:w_update}), but we neglect the $x_i$:
\begin{equation}
w_{ij}^{\dagger}=w_{ij} - \eta\cdot \frac{\partial E_{TOT}}{\partial w_{ij}}.
\end{equation}
To work the partial derivative out, we need to do a change of variables. I will focus on a 2-layer network, but the principle is the same for networks of more layers. The method will be slightly different for the two set of weights (W1 and W2), so I will do it seperately for them. We start with W2, which is naturally since we move backwards (hence backward propagation). 
\begin{align}
\frac{\partial E_{TOT}}{\partial w_{ij}}&=\frac{\partial E_{TOT}}{\partial out_{oj}}\cdot\frac{\partial out_{oj}}{\partial net_{oj}}\cdot\frac{\partial net_{oj}}{\partial w_{ij}}\\
&=\delta_{oj}\cdot\frac{\partial net_{oj}}{\partial w_{ij}}
\end{align}
where $\delta_{oj}$ is the same for all weights that go to the same output $oj$ $\Rightarrow$ we will have $O$ different $\delta_{oj}$'s. Further recall the error function from equation (\ref{eq:error_function}), and recognize that
\begin{equation}
\frac{\partial E_{TOT}}{\partial out_{oj}}=-(t_{oj}-out_{oj})
\end{equation}
where $t_{oj}$ are the targets. Then we have the sigmoid function where we get $out_{oj}$ when we send in $net_{oj}$:
\begin{equation}
out_{oj}=\frac{1}{1+\exp(-net_{oj})}
\end{equation}
such that
\begin{equation}
\frac{\partial out_{oj}}{\partial net_{oj}}=out_{oj}(1-out_{oj})
\end{equation}
as mentioned in section 1 and proven in Appendix A. We end up with
\begin{equation}
\delta_{oj}=-(t_{oj}-out_{oj})\cdot out_{oj}(1-out_{oj})=\frac{\partial E_{TOT}}{\partial net_{oj}}.
\end{equation}
We also have that
\begin{equation}
net_{oj}=\sum_k w_{kj}\cdot out_{hk} + b_{2j}\cdot 1
\end{equation}
such that
\begin{equation}
\frac{\partial net_{oj}}{\partial w_{ij}}=out_{hi}
\end{equation}
and 
\begin{equation}
\frac{\partial E_{TOT}}{\partial w_{ij}}=\delta_{oj}\cdot out_{hi}.
\end{equation}
The specific updating algorithm is the following
\begin{empheq}[box={\mybluebox[5pt]}]{equation}
w_{ij}^{(2)}\rightarrow w_{ij}^{(2)} - \eta\cdot\delta_{oj}\cdot out_{hi}
\end{empheq}
with 
\begin{equation*}
\delta_{oj}=-(t_{oj}-out_{oj})\cdot out_{oj}(1-out_{oj})
\end{equation*}
We are now set for updating the last weights W2, see section \ref{sec:multi_algorithm} for a overview of the algorithm, section \ref{sec:vectorization} for a short description of how to vectorize the algorithm, or if you want to go straight to the implementation, you should check out section \ref{sec:multi_implementation}. 

However, we also need to update the remaining weights, W1, what are we waiting for? The approach is the same as above with considering how much the total error will change when changing one of the weights and doing a change of variables
\begin{equation}
\frac{\partial E_{TOT}}{\partial w_{ij}}=\frac{\partial E_{TOT}}{\partial out_{hj}}\cdot\frac{\partial out_{hj}}{\partial net_{hj}}\cdot\frac{\partial net_{hj}}{\partial w_{ij}}.
\end{equation}
A difference is that the error for each of the hidden nodes is dependent on the error at each output nodes, such that now $E_{TOT}$ needs to be split up in $O$ terms
\begin{equation}
E_{TOT}=E_{o1} + E_{o2} + \hdots + E_{oO}.
\end{equation}
This causes
\begin{equation}
\frac{\partial E_{TOT}}{\partial out_{hj}}=\frac{\partial E_{o1}}{\partial out_{hj}} + \frac{\partial E_{o2}}{\partial out_{hj}} + \hdots + \frac{\partial E_{oO}}{\partial out_{hj}}
\end{equation}
where we again need to do a change of variables on each of those
\begin{equation}
\frac{\partial E_{ok}}{\partial out_{hj}}=\frac{\partial E_{ok}}{\partial net_{ok}}\cdot\frac{\partial net_{ok}}{\partial out_{hj}}.
\end{equation}
We could have done another change of variables above to avoid equation (\ref{eq:dEokdnetoj}), but this is neater
\begin{equation}
\frac{\partial E_{ok}}{\partial net_{ok}}=\frac{\partial E_{ok}}{\partial out_{ok}}\cdot \frac{\partial out_{ok}}{\partial net_{ok}}.
\label{eq:dEokdnetoj}
\end{equation}
Now it gets more similar to the first weight updates again:
\begin{equation}
\frac{\partial E_{ok}}{\partial out_{ok}}=-(t_{ok}-out_{ok})
\end{equation}
and
\begin{equation}
\frac{\partial out_{ok}}{\partial net_{ok}} = out_{ok}(1-out_{ok}).
\end{equation}
Further we need to calculate $\partial net_{ok}/\partial out_{hj}$, which is done by
\begin{equation}
net_{ok} = \sum_i w_{ik}^{(2)}\cdot out_{hi} + b_{2k}\cdot 1
\end{equation}
and
\begin{equation}
\frac{\partial net_{ok}}{\partial out_{hj}}=w_{jk}^{(2)}
\end{equation}
and we end up with
\begin{equation}
\frac{\partial E_{ok}}{\partial out_{hj}}=-(t_{ok}-out_{ok})\cdot out_{ok}(1-out_{ok})\cdot w_{jk}^{(2)}.
\end{equation}
That was the difficult part, as we have seen before
\begin{equation}
\frac{\partial out_{hj}}{\partial net_{hj}}=out_{hj}(1-out_{hj})
\end{equation}
and since
\begin{equation}
net_{hj} = \sum_l w_{lj}\cdot x_l
\end{equation}
where $x_i$ is input $i$, we obtain
\begin{equation}
\frac{\partial net_{hj}}{\partial w_{ij}}=x_i
\end{equation}
and we finally end up with 
\begin{equation}
\frac{\partial E_{TOT}}{\partial w_{ij}^{(1)}}=\sum_{k=1}^{O}-(t_{ok}-out_{ok})\cdot out_{ok}(1-out_{ok})\cdot w_{jk}^{(2)}\cdot out_{hj}(1-out_{hj})\cdot x_i.
\end{equation}
We recognize the first part as $\delta_{ok}$, such that
\begin{empheq}[box={\mybluebox[5pt]}]{equation}
w_{ij}^{(1)}\rightarrow w_{ij}^{(1)} - \eta\cdot\sum_{k=1}^{O}\delta_{ok}\cdot w_{jk}^{(2)}\cdot out_{hj}(1-out_{hj})\cdot x_i
\end{empheq}
where we recall $\delta_{ok}$ as
\begin{equation*}
\delta_{ok}=-(t_{ok}-out_{ok})\cdot out_{ok}(1-out_{ok}).
\end{equation*}