\section{Discussion} \label{sec:discussion}
In table \eqref{tab:lin_reg}, one can see that the obtained results are quite satisfying, especially for Ridge and Lasso. However, I obviously do not get the same results as for Metha et. al. [3], who over all got a small MSE for the training set, but could not keep it up for the test set using OLS and Ridge. I have checked if the train and test sets are mixed up somewhere, but could not find the error.

Further, we observe that only Lasso is able to obtain the correct J-matrix, which is because it does not conserve symmetries. For OLS and Ridge, we get sub-superdiagonal matrices, with values 0.5 instead of 1.0. The result is an interaction 0.5 between particle $i$ and $j$, and then an interaction 0.5 between $j$ and $i$, which should give the same as just an interaction 1.0 between $i$ and $j$ since we apply periodic boundary conditions. In figure \eqref{fig:lambda_vs_R2_linear}, we can see that the lower penalty the better, which we expect to be valid until the models approach OLS. 

When applying the neural network, linear cost function on the hidden nodes gives the lowest MSE because the function is linear below x=0 as well. The more nodes we get, the lower MSE we get for this function. The remaining functions are quite consistent, probably because they have similar form (low but nonzero derivative below zero), but they give best results without any hidden layer, which is linear regression. We also tried multiple layers, but apparently a model with a hidden layer is complex enough.
\vspace{1cm}

For classification, the logistic regression did not work well, with a maximum accuracy of 0.73. The spin configurations are not linear with the phase, such that a model without a hidden layer is not complex enough. In figure \eqref{fig:class_logistic}, we also see that the model is very sensitive, and the MSE is jumping up and down as we vary the regularization and learning rate. An optimal learning rate is not too small, neither too large, which is as expected since we often will walk past the minimum if the learning rate is too large, and it takes time to reach the minimum if the learning rate is small. We also observe that we get relatively good accuracy for some specific values of $\lambda$ and $\eta$ for the critical set, something I believe happen by accident.

Unlike when we applied a neural network on regression, using a pure linear activation function on the hidden layer(s) causes a poor performance. On the other hand, the ReLU and leaky ReLU activation functions give surprisingly good results with accuracy above 99\%! The reason could be that the the derivative below zero is much lower for the latter functions, which gives a faster training. [6] Further, we observe that the accuracy slightly increase when adding more layers, an effect that probably could be recreated by more hidden nodes in one layer according to the universal approximation theorem. An accuracy of 99.9\% for the critical set is also higher than expected, and shows that there is a distinctive difference between the ordered and disordered lattices also close to the critical temperature. 

In the end, I added a plot which shows how the R$^2$-score depends on the number of hidden nodes. Apparently, the accuracy score is stable above a certain number of nodes for both one and two hidden layers, so perhaps it does not matter how many hidden nodes we use as long as we are above this unstable area. 

