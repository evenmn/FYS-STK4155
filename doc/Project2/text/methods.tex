\section{Methods} \label{sec:methods}

\subsection{Resampling techniques} \label{sec:resampling}
A resampling technique is a way of estimating the variance of data sets without calculating the covariance. As we saw in section section \ref{sec:error_analysis}, the true covariance is given by a double sum which we will avoid calculating if possible. There are different ways of doing this, and we have already went through several of them in this course:
\begin{itemize}
	\item{Jackknife resampling}
	\item{K-fold validation}
	\item{Bootstrap method}
	\item{Blocking method}.
\end{itemize}

For this particular project we have been focusing on the bootstrap and the K-fold validation methods, so only they will be covered here.

\subsubsection{Bootstrap method} \label{sec:bootstrap}
When we construct a data set, we usually draw samples from a probability density function (PDF) and get a set of samples $\vec{x}$. If we draw a large number of samples, the sample variance will approach the variance of the PDF. The bootstrap method turns this upside down, and tries to estimate the PDF given a set data set, because if we know the PDF, we know in principle everything about the data set. 

The assumption we need to make, is that the relative frequency of $x_i$ equals $p(x_i)$, which is reasonable (for instance, think about how the histogram looks like when we draw samples from a normal distribution). In this project the vector that we want to find, $\vec{\beta}(x,y)$, is a function of two set of variables. Fortunately, they are independent of eachother, so we can safely apply the independent bootstrap on them separately. The independent bootstrap goes as
\begin{enumerate}
	\item Draw $n$ samplings from the data set $\vec{x}$ with replacement and denote the new data set as $\vec{x}^*=\{x_1,x_2,\hdots,x_n\}$
	\item Compute $\vec{\beta}(\vec{x}^*)\equiv\vec{\beta}^*$
	\item Repeat the procedure above $K$ times
	\item The average value of all $K$ $\vec{\beta}^*$'s are stored in a new vector $\vec{\bar{\beta}}^*$
	\item Finally, the variance of $\vec{\bar{\beta}}^*$ should correspond to the sample variance
\end{enumerate}
[9]

The implementation could look something like this
\lstset{basicstyle=\scriptsize}
\begin{lstlisting}
def bootstrap(data, K=1000):
dataVec = np.zeros(K)
for k in range(K):
dataVec[k] = np.average(np.random.choice(data, len(data)))
Avg = np.average(dataVec)
Var = np.var(dataVec)
Std = np.std(dataVec)

return Avg, Var, Std
\end{lstlisting}
which is strongly inspired by the lecture notes. [10]

It is also worth to mention that we should hold back a small part of the original data set for testing, since we need to test the model on data that it has not seen before.

\subsubsection{K-fold validation method} \label{sec:kfold}
K-fold validation is a method that has more describing name than many of its fellow methods. The idea is to make the most use of our data by splitting it into $k$ folds and training $k$ times on it. Every time we train, we leave out one of the folds, which gonna be our validation data. This validation data needs to be different every time, and we are therefore restricted to $k$ unique training sessions. A typical overview looks like this:
\begin{itemize}
	\item Split data set into $k$ equally sized folds
	\item Use the $k-1$ first folds as training data, and leave the $k$'th fold for validation
	\item Use the $k-2$ first folds plus the $k$-th fold as training data, and leave the $(k-1)$'th fold for validation
	\item Continue until all folds are used as training data
\end{itemize}
[7]. An example implementation of K-fold validation, and in fact the function used in this project, can be seen below.

\lstset{basicstyle=\scriptsize}
\begin{lstlisting}
def k_fold(x, y, z, K=10):
xMat = np.reshape(x, (K, int(len(x)/K)))      # Reshape x-coords
yMat = np.reshape(y, (K, int(len(y)/K)))      # Reshape y-coords
zMat = np.reshape(z, (K, int(len(z)/K)))      # Reshape z-coords

MSE_train = 0	                                # Declare MSE_train variable
MSE_test  = 0	                                # Declare MSE test variable
R2_train  = 0	                                # Declare R2 train variable
R2_test   = 0                                 # Declare R2 test variable

for i in range(K):
xMatNew = np.delete(xMat, i, 0)             # Ignore test sets
yMatNew = np.delete(yMat, i, 0)             # Ignore test sets
zMatNew = np.delete(zMat, i, 0)             # Ignore test sets

xVecNew = xMatNew.flatten()                 # Reshape training sets
yVecNew = yMatNew.flatten()                 # into vectors
zVecNew = zMatNew.flatten()

order5 = Reg_2D(xVecNew, yVecNew, zVecNew, Px=5, Py=5)
beta_train = order5.ols()

MSE_train += MSE(xVecNew, yVecNew, zVecNew, beta_train)
MSE_test += MSE(xMat[i], yMat[i], zMat[i], beta_train)

R2_train += R2(xVecNew, yVecNew, zVecNew, beta_train)
R2_test += R2(xMat[i], yMat[i], zMat[i], beta_train)

return MSE_train/K, MSE_test/K, R2_train/K, R2_test/K
\end{lstlisting}

\subsection{Minimization methods}\label{sec:minimization}
Suppose we have a very simple model trying to fit a straight line to data points. In that case, we could manually vary the coefficients and find a line that fits the points quite good. However, when the model gets more complicated, this can be a time consuming activity. Would it not be good if the program could do this for us?

In fact there are multiple techniques for doing this, where the most complicated ones obviously also are the best. Anyway, in this project we will have good initial guesses, and are therefore not in need for the most fancy algorithms. We will stick to gradient descent in this project, which might be the simplest method for our purposes.

\subsubsection{Gradient Descent} \label{sec:gd}
The Gradient Descent method was mentioned already in the theory part, in equation \eqref{eq:w_update}, and this will therefore just be a quick reminder of the idea. 
Perhaps the simplest and most intuitive method for finding the minimum is the gradient descent method (GD), which reads
\begin{empheq}[box={\mybluebox[5pt]}]{equation}
\label{eq:GD}
\beta_i^{\text{new}}=\beta_i - \eta\cdot\frac{\partial Q(\beta_i)}{\partial\beta_i}
\end{empheq}
where $\beta_i^{\text{new}}$ is the updated $\beta$ and $\eta$ is a step size, in machine learning often refered to as the learning rate. The idea is to find the gradient of the cost function $Q(\vec{\beta})$ with respect to a certain $\beta_i$, and move in the direction which minimizes the cost function. This is repeated until a minimum is found, defined by either
\begin{equation}
Q(\beta_i)<\varepsilon
\end{equation}
or that the change in $\beta_i$ for the past $x$ steps is small. 
The algorithm for this minimization method is thus as follows:

\lstset{basicstyle=\scriptsize}
\begin{lstlisting}
while dbeta > epsilon:
e = z - X.dot(beta)
debeta = 2*X.T.dot(e) -np.sign(beta)*q*lambda*np.power(abs(beta), q-1)
beta += eta*dbeta
\end{lstlisting}

\subsubsection{Stochastic Gradient Descent (SGD)}
It will probably not converge in fewer steps, but each step will be faster.
\begin{empheq}[box={\mybluebox[5pt]}]{equation}
\label{eq:SGD}
\beta_i^{\text{new}}=\beta_i - \eta\cdot\frac{\partial Q(\beta_i)}{\partial\beta_i}
\end{empheq}

\subsection{Error analysis}
\label{sec:error_analysis}
To find out how good the fit is, we could potentially compare the polynomial plot to the data plot and decide whether the fit is good or not. However, that approach is risky in the sense that it is hard to determine how good a fit really is, and we therefore rely on error estimates. There are many ways to calculate the error, where some frequently used methods are
\begin{itemize}
	\item{the absolute error}
	\item{the relative error}
	\item{the mean square error (MSE)}
	\item{the R$^2$ score function}
	\item{the accuracy score function}
\end{itemize}
In this report we will study only the MSE and R$^2$ score function.

\subsubsection{Mean Square Error (MSE)} \label{sec:MSE}
The most popular error estimation method is the mean square error, also called least squares. We study the MSE in order to compute the reduction of the cost function, because it is basically the standard cost function, used in OLS. 
\begin{empheq}[box={\mybluebox[5pt]}]{equation}
\text{MSE}(\vec{\beta})=\frac{1}{N}\sum_{i=1}^N(y_i-t_i)^2
\end{empheq}
Compared to least absolute value, the points far away from the fitted line are weighted stronger. 

\subsubsection{R$^2$ score function} \label{sec:R2}
The R$^2$ score function is a measure of how close the data are to the fitted regression line, and is a widely used quantity within statistics. In entirety it reads
\begin{empheq}[box={\mybluebox[5pt]}]{equation}
R^2(\vec{y},\tilde{\vec{y}})=1-\frac{\sum_{i=1}^N(y_i-t_i)^2}{\sum_{i=1}^N(y_i-\bar{y})^2}.
\end{empheq}
and is a ratio between the explained variation and total variation. If it is, on one hand, able to explain all the variations the score is 1, which is the best. On the other hand, if it is not able to describe any of the variations, the R$^2$-score is low. We are therefore fighting for a high R$^2$-score.

\subsubsection{Accuracy score}
The accuracy score is a very intuitive error estimation, and is just number of correctly classified images divided by the total number of images,
\begin{empheq}[box={\mybluebox[5pt]}]{equation}
\text{Accuracy}=\frac{\sum_{i=1}^nI(y_i=t_i)}{n}.
\end{empheq}
It is typically used to determine the error in classification, since the error is binary (the classification is either correct, or wrong).