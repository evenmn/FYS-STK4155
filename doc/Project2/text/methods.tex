\section{Methods} \label{sec:methods}

\subsection{Resampling techniques} \label{sec:resampling}
A resampling technique is a way of estimating the variance of data sets without calculating the covariance, which is very expensive calculating. There are various methods for this, including the K-fold cross-validation, Bootstrap and Blocking methods. In this particular project, we will focus on the K-fold cross-validation.

\subsubsection{K-fold validation method} \label{sec:kfold}
K-fold validation is a method which has more describing name than many of its fellow methods. The idea is to make the most use of our data by splitting it into $K$ folds and train our model $K$ times on it. Every time we train, we leave out one of the folds, which gonna be our validation data. This validation data needs to be different every time, and we are therefore restricted to $K$ unique training sessions. A typical overview looks like this:
\begin{itemize}
	\item Split data set into $K$ equally sized folds
	\item Use the $K-1$ first folds as training data, and leave the $K$'th fold for validation. Then calculate the MSE and the R$^2$-score of the training and test set.
	\item Use the $K-2$ first folds plus the $K$-th fold as training data, and leave the $(K-1)$'th fold for validation. Calculate the MSE and R$^2$-score of training and test set
	\item Continue until all folds are used as training data
	\item Return the average training and test MSE, and the average training and test R$^2$-score. Typically one is interested in the average test errors. 
\end{itemize}
[7]. An example implementation of K-fold validation, and in fact the function used in the regression case, can be seen below.

\lstset{basicstyle=\scriptsize}
\begin{lstlisting}
def k_fold(X, E, T, h, eta, K):
	'''K-fold validation resampling based on neural netowrk'''
	
	MSE_train = 0
	MSE_test = 0
	R2_train = 0
	R2_test = 0
	
	Xmat = np.reshape(X, (K, int(len(X)/K), len(X[0])))
	Emat = np.reshape(E, (K, int(len(X)/K)))
	
	for i in range(K):
		Xnew = np.delete(Xmat, i, 0)
		Enew = np.delete(Emat, i, 0)
		
		X_train = np.reshape(Xnew, (len(Xnew)*len(Enew[0]), len(X[0])))
		E_train = np.reshape(Enew, (len(Xnew)*len(Enew[0])))
		
		obj = nn.NeuralNetwork(X_train, E_train, T, h, eta)
		W = obj.solver()
		E_train_tilde = obj.recall(X_train)
		E_test_tilde = obj.recall(Xmat[i])
	
	MSE_train += MSE(E_train_tilde, E_train)
	MSE_test += MSE(E_test_tilde, Emat[i])
	
	R2_train += R2(E_train_tilde, E_train)
	R2_test += R2(E_test_tilde, Emat[i])
	
	return MSE_train/K, MSE_test/K, R2_train/K, R2_test/K
\end{lstlisting}

\subsection{Minimization methods}\label{sec:minimization}
Suppose we have a very simple model trying to fit a straight line to data points. In that case, we could manually vary the coefficients and find a line that fits the points quite good. However, when the model gets more complicated, this can be a time consuming activity. Would it not be good if the program could do this for us?

In fact there are multiple techniques for doing this, where the most complicated ones obviously also are the best. Anyway, in this project we will have good initial guesses, and are therefore not in need for the most fancy algorithms. We will stick to gradient descent in this project, which might be the simplest method for our purposes.

\subsubsection{Gradient Descent} \label{sec:gd}
The Gradient Descent method was mentioned already in the theory part, in equation \eqref{eq:w_update}, and this will therefore just be a quick reminder of the idea. 
Perhaps the simplest and most intuitive method for finding the minimum is the gradient descent method (GD), which reads
\begin{empheq}[box={\mybluebox[5pt]}]{equation}
\label{eq:GD}
\beta_i^{\text{new}}=\beta_i - \eta\cdot\frac{\partial Q(\beta_i)}{\partial\beta_i}
\end{empheq}
where $\beta_i^{\text{new}}$ is the updated $\beta$ and $\eta$ is a step size, in machine learning often refered to as the learning rate. The idea is to find the gradient of the cost function $Q(\vec{\beta})$ with respect to a certain $\beta_i$, and move in the direction which minimizes the cost function. This is repeated until a minimum is found, defined by either
\begin{equation}
Q(\beta_i)<\varepsilon
\end{equation}
or that the change in $\beta_i$ for the past $x$ steps is small. 
The algorithm for this minimization method is thus as follows:

\lstset{basicstyle=\scriptsize}
\begin{lstlisting}
while dbeta > epsilon:
e = z - X.dot(beta)
debeta = 2*X.T.dot(e) -np.sign(beta)*q*lambda*np.power(abs(beta), q-1)
beta += eta*dbeta
\end{lstlisting}

\subsubsection{Stochastic Gradient Descent (SGD)}
It will probably not converge in fewer steps, but each step will be faster.
\begin{empheq}[box={\mybluebox[5pt]}]{equation}
\label{eq:SGD}
\beta_i^{\text{new}}=\beta_i - \eta\cdot\frac{\partial Q(\beta_i)}{\partial\beta_i}
\end{empheq}

\subsection{Error analysis}
\label{sec:error_analysis}
To find out how good the fit is, we could potentially compare the polynomial plot to the data plot and decide whether the fit is good or not. However, that approach is risky in the sense that it is hard to determine how good a fit really is, and we therefore rely on error estimates. There are many ways to calculate the error, where some frequently used methods are
\begin{itemize}
	\item{the absolute error}
	\item{the relative error}
	\item{the mean square error (MSE)}
	\item{the R$^2$ score function}
	\item{the accuracy score function}
\end{itemize}
In this report we will study only the MSE and R$^2$ score function.

\subsubsection{Mean Square Error (MSE)} \label{sec:MSE}
The most popular error estimation method is the mean square error, also called least squares. We study the MSE in order to compute the reduction of the cost function, because it is basically the standard cost function, used in OLS. 
\begin{empheq}[box={\mybluebox[5pt]}]{equation}
\text{MSE}(\vec{\beta})=\frac{1}{N}\sum_{i=1}^N(y_i-t_i)^2
\end{empheq}
Compared to least absolute value, the points far away from the fitted line are weighted stronger. 

\subsubsection{R$^2$ score function} \label{sec:R2}
The R$^2$ score function is a measure of how close the data are to the fitted regression line, and is a widely used quantity within statistics. In entirety it reads
\begin{empheq}[box={\mybluebox[5pt]}]{equation}
R^2(\vec{y},\tilde{\vec{y}})=1-\frac{\sum_{i=1}^N(y_i-t_i)^2}{\sum_{i=1}^N(y_i-\bar{y})^2}.
\end{empheq}
and is a ratio between the explained variation and total variation. If it is, on one hand, able to explain all the variations the score is 1, which is the best. On the other hand, if it is not able to describe any of the variations, the R$^2$-score is low. We are therefore fighting for a high R$^2$-score.

\subsubsection{Accuracy score}
The accuracy score is a very intuitive error estimation, and is just number of correctly classified images divided by the total number of images,
\begin{empheq}[box={\mybluebox[5pt]}]{equation}
\text{Accuracy}=\frac{\sum_{i=1}^nI(y_i=t_i)}{n}.
\end{empheq}
It is typically used to determine the error in classification, since the error is binary (the classification is either correct, or wrong).